{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f7f7247",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np, xarray as xr, matplotlib.pylab as plt, pandas as pd, seaborn as sns\n",
    "import pickle, random, re, io, regionmask, dask, timeit, os, shutil, datetime\n",
    "from scipy.signal import welch; from eofs.xarray import Eof; import matplotlib as mpl\n",
    "\n",
    "from statsmodels.tsa.api import VAR; from contextlib import redirect_stdout\n",
    "from distributed import Client; from scipy import stats\n",
    "import cartopy.crs as ccrs, cartopy.feature as cfeature\n",
    "import matplotlib.gridspec as gridspec; import cartopy.feature as cfeature\n",
    "from matplotlib.colors import Normalize; import matplotlib.pyplot as plt\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "from InVERT_functions import (calc_weights, concat_with_monthids, calc_EOFs, areaweighted_mean, \n",
    "autocorr, unstack_time, stack_time, createRandomSortedList, compare_T_pdfs, calc_psd_stats, \n",
    "plot_GMST_psd_spread, calc_emean_autocorrs, calc_ensemble_std_autocorrs, calc_efold_time, \n",
    "calc_eft_stats, compare_autocorrs_emean, plot_GMST_comparisons, save_region_means,\n",
    "plot_regional_psd_spread, plot_regional_T_pdfs, plot_regional_emean_autocorrs, \n",
    "plot_regional_comparison, find_var_name, compare_MSE_to_emean_PSD, welch_psd, get_ensemble_variance, \n",
    "calc_emean_gridcell_MSE, plot_regional_variance_stats, plot_regional_eft_stats,\n",
    "gridcell_map_plot, plot_gridcell_diff, plot_regional_diff_map, calc_gridcell_psd, plot_MSE_by_region,\n",
    "calc_efold_time_dataset, plot_var_coeffs, convert_lon, emulate_pcs, plot_local_monthly_T_stds,\n",
    "calc_gridcell_monthly_autocorrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10c12498",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = 'Historical'\n",
    "\n",
    "savepath = '/home/msaenger/InVERT/output/' # directory for saving InVERT output and training EOF data\n",
    "    \n",
    "lpath = '/home/msaenger/LENS2_' + scenario + '/TREFHT/' # directory where LENS2 training data is saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ded9684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 25   # number of InVERT ensemble members to generate\n",
    "n_steps = 1980   # number of time steps (months) to generate per InVERT ensemble member \n",
    "optimal_lag = 12 # VAR model lag (months)\n",
    "\n",
    "nmodes = 100    # number of EOF modes to include\n",
    "M = 120          # Number of initial time steps (months) to truncate for spin-up (aka 'burn-in' period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63e9e646",
   "metadata": {},
   "outputs": [],
   "source": [
    "LENS_esize = 50  # Size of LENS ensemble (number of members)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c70028de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure formatting\n",
    "mpl.rcParams['font.family'] = 'sans-serif' # Change the default font family\n",
    "tickfontsize = 14; axislabelfontsize=16\n",
    "titlefontsize=18; legendfontsize=14\n",
    "color1 = 'goldenrod'; color2='teal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cfdf7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if scenario == 'Historical':\n",
    "    scenario_name = 'HIST'\n",
    "else: scenario_name = scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a486d761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### DATA PREPROCESSING ####\n",
    "\n",
    "# path = '/home/msaenger/CESM_LENS2/' + scenario + \\\n",
    "#        '/TREFHT/combined_by_ensemble_member/regridded/'\n",
    "\n",
    "# ## combine 50 regridded ensemble member files into one dataset with 'ensemble' dimension\n",
    "# ds = xr.open_mfdataset(path + '*.nc', concat_dim='ensemble', \n",
    "#                          combine='nested', parallel=True)\n",
    "\n",
    "# ## assign ensemble coordinate\n",
    "# ds = ds.assign_coords(ensemble=np.arange(len(ds['ensemble'])))\n",
    "\n",
    "# ## Calculate ensemble mean and add to dataset as a variable\n",
    "# ds['emean'] = ds.TREFHT.mean('ensemble')\n",
    "\n",
    "\n",
    "# ## subtract ensemble mean to get anomalies \n",
    "# anoms = ds['TREFHT'] - ds['emean']\n",
    "\n",
    "# anoms = anoms.to_dataset(name='anoms')\n",
    "# anoms['gmean'] = areaweighted_mean(anoms.anoms)\n",
    "# anoms.to_netcdf(lpath + 'LENS2_TREFHT_anomalies_regridded.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da032ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "anoms = xr.open_dataset(lpath + 'LENS2_TREFHT_anomalies_regridded.nc')\n",
    "\n",
    "# anoms_concatted = concat_with_monthids(anoms)\n",
    "\n",
    "# anoms_concatted['gmean'] = areaweighted_mean(anoms_concatted.anoms)\n",
    "\n",
    "# ## Save month IDs from original T anomaly time series\n",
    "# month_da = xr.DataArray(anoms_concatted.month.values,\n",
    "#                         coords={'time': anoms_concatted.time.values, \n",
    "#                                 'month': ('time', anoms_concatted.month.values)},\n",
    "#                         dims=['time'])\n",
    "\n",
    "# anoms_concatted['month'] = month_da\n",
    "\n",
    "# ## Save anomalies from ensemble mean that don't have monthly climatologies removed\n",
    "\n",
    "# anoms_concatted.to_netcdf(lpath + 'LENS2_TREFHT_anomalies_regridded_concatted.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5af63c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomly select a subset of ensemble members train on\n",
    "\n",
    "n_training_members = 25 # Number of training ensemble members to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49cf579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_members = (createRandomSortedList(n_training_members))\n",
    "\n",
    "print('training members:', training_members)\n",
    "\n",
    "training_anoms = anoms.sel(ensemble = [ens for ens in training_members])\n",
    "\n",
    "training_anoms.to_netcdf(savepath + 'LENS2_Tanoms_' + str(n_training_members) + '_training_members.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d0892f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatenate training ensemble members over time dimension and save\n",
    "\n",
    "training_anoms_concatted = concat_with_monthids(training_anoms.anoms)\n",
    "\n",
    "## Save month IDs from original T anomaly time series\n",
    "month_da = xr.DataArray(training_anoms_concatted.month.values,\n",
    "                        coords={'time': training_anoms_concatted.time.values, \n",
    "                                'month': ('time', training_anoms_concatted.month.values)},\n",
    "                        dims=['time'])\n",
    "\n",
    "training_anoms_concatted['month'] = month_da\n",
    "training_anoms_concatted['gmean'] = areaweighted_mean(training_anoms_concatted.anoms)\n",
    "\n",
    "training_anoms_concatted.to_netcdf(savepath + \\\n",
    "                        'LENS2_concatted_Tanoms_25_training_members.nc')\n",
    "\n",
    "month_da.to_netcdf(savepath + 'month_da.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3947c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save non-training ensemble members for diagnostics\n",
    "\n",
    "testing_ensembles = [i for i in np.arange(0,50) if i not in training_members]\n",
    "\n",
    "testing_anoms = anoms.sel(ensemble = random.sample(testing_ensembles, 25))\n",
    "\n",
    "print('testing members:', testing_anoms.ensemble.values)\n",
    "                         \n",
    "testing_anoms.to_netcdf(savepath + 'LENS2_Tanoms_25_testing_members.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aca5cad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatenate non-training ensemble members over time dimension and save \n",
    "\n",
    "testing_anoms_concatted = concat_with_monthids(testing_anoms.anoms)\n",
    "\n",
    "## Save month IDs from original T anomaly time series\n",
    "month_da = xr.DataArray(testing_anoms_concatted.month.values,\n",
    "                        coords={'time': testing_anoms_concatted.time.values, \n",
    "                                'month': ('time', testing_anoms_concatted.month.values)},\n",
    "                        dims=['time'])\n",
    "\n",
    "testing_anoms_concatted['month'] = month_da\n",
    "testing_anoms_concatted['gmean'] = areaweighted_mean(testing_anoms_concatted.anoms)\n",
    "\n",
    "testing_anoms_concatted.to_netcdf(savepath + \\\n",
    "            'LENS2_concatted_Tanoms_25_testing_members.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5650228a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate month-specific EOFs from training ensemble members\n",
    "\n",
    "for month in range(1,13):\n",
    "\n",
    "    month_EOFs = calc_EOFs(training_anoms_concatted.groupby('month')[month].anoms, path=savepath,\n",
    "                      filename = 'LENS2_'+scenario_name+'_monthly_Tanom_EOFs_month='+str(month))\n",
    "    print('month ' + str(month) + ' EOFs saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c848ebde",
   "metadata": {},
   "source": [
    "## Run emulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15449ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EOF solver\n",
      "Loading EOF solver\n",
      "Loading EOF solver\n",
      "Loading EOF solver\n",
      "Loading EOF solver\n",
      "Loading EOF solver\n",
      "Loading EOF solver\n",
      "Loading EOF solver\n",
      "Loading EOF solver\n",
      "Loading EOF solver\n",
      "Loading EOF solver\n",
      "Loading EOF solver\n"
     ]
    }
   ],
   "source": [
    "# ## Load EOF solvers by month from training ensemble members\n",
    "\n",
    "solvers_bymonth = {}\n",
    "for month in range(1,13):\n",
    "    solvers_bymonth[month] = calc_EOFs(0, path=savepath, filename='LENS2_'+scenario_name + \\\n",
    "                                       '_monthly_Tanom_EOFs_month='+str(month))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3770104",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract and save PCs, EOFs, and variance fractions from the EOF solver object\n",
    "\n",
    "eofs_dict = {}\n",
    "for month in range(1, 13):\n",
    "    eofs_dict[month] = {}\n",
    "    eofs_dict[month]['eofs'] = solvers_bymonth[month].eofs().sel(mode=slice(0, nmodes-1))\n",
    "    eofs_dict[month]['pcs'] = solvers_bymonth[month].pcs().sel(mode=slice(0, nmodes-1))\n",
    "    eofs_dict[month]['varfracs'] = solvers_bymonth[month].varianceFraction().sel(mode=slice(0, nmodes-1))\n",
    "    \n",
    "## Extract cos(lat) weights for later use\n",
    "\n",
    "weights = solvers_bymonth[1].getWeights()\n",
    "weights = xr.DataArray(weights, coords=[eofs_dict[1]['eofs']['lat'], \n",
    "                                        eofs_dict[1]['eofs']['lon']], \n",
    "                       dims=['lat', 'lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54849070",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compile DataArrays of PCs from the EOF solvers for each month\n",
    "\n",
    "## Save PC data array (separated by ensemble member, e.g. 'unstacked') for each month and store in dictionary\n",
    "\n",
    "pcs_unstacked = {}\n",
    "\n",
    "for month in range(1, 13):\n",
    "\n",
    "    pcs_unstacked[month] = unstack_time(eofs_dict[month]['pcs'].drop('month'), \n",
    "                                        esize = n_training_members) \n",
    "    \n",
    "# Convert dictionary to xarray dataset. Variable names will be the month (1-12)\n",
    "\n",
    "training_pcs_bymonth_unstacked = xr.Dataset(pcs_unstacked)\n",
    "\n",
    "# Re-stack (e.g. concatenate) the training ensemble members over time\n",
    "\n",
    "training_pcs_bymonth = stack_time(training_pcs_bymonth_unstacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de32fc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Extract each month's PC data array and adjust the 'time' values so as to put them \n",
    "# ## back together in time order (e.g. month 1 year 1, month 2 year 1, ... month 12 year 1, \n",
    "# ## month 1 year 2, month 2 year 2, ... etc)\n",
    "\n",
    "month_pc_da_list = []\n",
    "\n",
    "for month in range(1, 13):\n",
    "\n",
    "    training_pcs_da_month = training_pcs_bymonth[month].drop('ensemble')\n",
    "    training_pcs_da_month['time'] = training_pcs_da_month.time * 12 + month - 1\n",
    "    training_pcs_da_month = training_pcs_da_month.to_dataset(name='pcs')\n",
    "    month_pc_da_list.append(training_pcs_da_month)\n",
    "    \n",
    "# ## Merge into one dataset, sorted by time, and re-apply month coordinate\n",
    "\n",
    "month_da = xr.open_dataarray(savepath + 'month_da.nc')\n",
    "\n",
    "training_pcs = xr.merge(month_pc_da_list).sortby('time')\n",
    "training_pcs['month'] = month_da.sel(time=slice(0,len(training_pcs['time'])))\n",
    "training_pcs = training_pcs.assign_coords({'month': training_pcs.month})\n",
    "\n",
    "# ## Save training PCs as netcdf \n",
    "training_pcs.to_netcdf(savepath + 'training_pcs.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d42bfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train 12 VAR models (one for each month of the year) using input-output pairs and standardization\n",
    "\n",
    "### monthly_var_models will contain regression coefficients and residuals for each monthly VAR model\n",
    "\n",
    "monthly_var_models = {}\n",
    "\n",
    "for target_month in range(1, 13): # Iterate through each target month (1 for Jan to 12 for Dec)\n",
    "\n",
    "    ## Lists to store input features and output targets for the current month \n",
    "    input_features = []; output_targets = []\n",
    "\n",
    "    ## Iterate through the time dimension of the original training data to create input-output pairs\n",
    "    ## Start the loop from optimal_lag so we have enough preceding data\n",
    "    \n",
    "    for i in range(optimal_lag, len(training_pcs.time)):\n",
    "        \n",
    "        ## Check if the current month is the target month\n",
    "        if training_pcs.month.values[i] == target_month:\n",
    "            \n",
    "            ## Extract the preceding optimal_lag consecutive months of standardized PC data\n",
    "            ## Flatten the data from (lag, mode) to a 1D array (lag * mode)\n",
    "            features = training_pcs.pcs.values[i - optimal_lag : i, :].flatten()\n",
    "            input_features.append(features)\n",
    "\n",
    "            ## Extract the current month's PCs as the output target\n",
    "            targets = training_pcs.pcs.values[i, :]\n",
    "            output_targets.append(targets)\n",
    "\n",
    "    ## Convert input and output feature lists to numpy arrays\n",
    "    input_features = np.array(input_features); output_targets = np.array(output_targets)\n",
    "\n",
    "    ## Check if enough data points to train the model\n",
    "    if len(input_features) > 0:\n",
    "        \n",
    "        ## Fit a linear regression model using the input-output pairs\n",
    "        ## Add a column of ones to input_features to account for the intercept (constant term)\n",
    "        X = np.hstack([np.ones((input_features.shape[0],1)), input_features])\n",
    "        y = output_targets\n",
    "\n",
    "        ## Solve for the coefficients using least squares regression\n",
    "        coefficients, residuals_info, rank, s = np.linalg.lstsq(X, y, rcond=None)\n",
    "\n",
    "        ## Calculate the predicted values for the training data\n",
    "        ## aka the linear combination (@) of the input features (lagged PCs and the intercept) \n",
    "            ## using the learned coefficients to produce the model's predicted PC values \n",
    "            ## for each instance of the target month in the training data\n",
    "        predicted_targets = X @ coefficients\n",
    "\n",
    "        ## Calculate the residuals (actual - predicted)\n",
    "        ## shape is the number of target_months in the training_pc time series minus 1\n",
    "            ## because the first instance of target_month won't have a full 12 months before it\n",
    "        residuals = y - predicted_targets\n",
    "\n",
    "        ## The first row of coefficients is the intercept of the lstsq calculation,\n",
    "            ## the rest are for the lagged coefficients\n",
    "        intercept = coefficients[0, :]\n",
    "        lagged_coeffs = coefficients[1:, :]\n",
    "\n",
    "        ## Store the VAR model components (coefficients, intercept, and residuals) for the target month\n",
    "        ## Reshape lagged_coeffs back to (lag, input_mode, output_mode)\n",
    "        monthly_var_models[target_month] = {\n",
    "            'intercept': intercept,\n",
    "            'lagged_coeffs': lagged_coeffs.reshape((optimal_lag, nmodes, nmodes)), \n",
    "            'residuals': residuals} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47d6e1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a list to store DataArrays for each month\n",
    "monthly_coeffs_da_list = []\n",
    "\n",
    "## Define coordinates\n",
    "lags_coord = np.arange(optimal_lag, 0, -1) # Lags from 12 down to 1 (e.g. 'months ago')\n",
    "input_modes_coord = np.arange(nmodes); output_modes_coord = np.arange(nmodes)\n",
    "\n",
    "## Iterate through each month in the monthly_var_models dictionary\n",
    "for month, components in monthly_var_models.items():\n",
    "    lagged_coeffs = components['lagged_coeffs']\n",
    "\n",
    "    ## Create a DataArray for the lagged coefficients of the current month\n",
    "    coeffs_da = xr.DataArray(lagged_coeffs,\n",
    "                             coords={'lag': lags_coord,\n",
    "                                     'input_mode': input_modes_coord,\n",
    "                                     'output_mode': output_modes_coord},\n",
    "                             dims=['lag', 'input_mode', 'output_mode'])\n",
    "\n",
    "    ## Add the month as a coordinate\n",
    "    coeffs_da = coeffs_da.expand_dims(month=[month])\n",
    "\n",
    "    monthly_coeffs_da_list.append(coeffs_da)\n",
    "\n",
    "## Concatenate the DataArrays along the new 'month' dimension\n",
    "lagged_coeffs_dataset = xr.concat(monthly_coeffs_da_list, dim='month')\n",
    "lagged_coeffs_dataset = lagged_coeffs_dataset.to_dataset(name='lagged_coefficients')\n",
    "lagged_coeffs_dataset.to_netcdf(savepath + 'lagged_coeff_dataset.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71470e6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Plot VAR coefficients\n",
    "\n",
    "plot_var_coeffs(lagged_coeffs_dataset, months_per_row=[1,2,3], savepath=savepath)\n",
    "plot_var_coeffs(lagged_coeffs_dataset, months_per_row=[4,5,6], savepath=savepath)\n",
    "plot_var_coeffs(lagged_coeffs_dataset, months_per_row=[7,8,9], savepath=savepath)\n",
    "plot_var_coeffs(lagged_coeffs_dataset, months_per_row=[10,11,12], savepath=savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72dfa7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Emulate PCs and save as netcdf\n",
    "InVERT_pcs = emulate_pcs(training_pcs, monthly_var_models, n_training_members,\n",
    "                         optimal_lag, n_samples, n_steps, nmodes, M, savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f398d492",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load emulated PCs\n",
    "InVERT_pcs = xr.open_dataset(savepath + 'InVERT_PCs.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e31e5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate PCs back into separate months, multiply by EOFs independently, and re-merge sorted by time\n",
    "\n",
    "## Compute PCs * EOFs for each mode and divide by weights for every month. Save in dict.\n",
    "\n",
    "print('Multiplying PCs * EOFs and dividing by weights')\n",
    "\n",
    "products_by_month = {}\n",
    "\n",
    "for month in range(1, 13):\n",
    "    print(month)\n",
    "    \n",
    "    products_by_month[month] = InVERT_pcs.groupby('month')[month] * eofs_dict[month]['eofs'] / weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86485e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sum T anomalies over modes then merge\n",
    "\n",
    "print('Summing over modes')\n",
    "\n",
    "products_by_month_summed = {}\n",
    "\n",
    "for month in range(1, 13):\n",
    "    \n",
    "    print(month)\n",
    "    \n",
    "    products_by_month_summed[month] =  products_by_month[month].pcs.sum(dim='mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12aca095",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Re-stack ensemble members over time dim (for easier re-sorting of months by time) and save in new dict\n",
    "\n",
    "print('Stacking ensemble members over time')\n",
    "\n",
    "Tanoms_bymonth = {}\n",
    "\n",
    "for month in range(1, 13):\n",
    "    print(month)\n",
    "    \n",
    "    Tanoms_bymonth[month] = (stack_time(products_by_month_summed[month]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "934acb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract each month's T anomaly data array and adjust the 'time' values so as to put them \n",
    "## back together in time order (e.g. month 1 year 1, month 2 year 1, ... month 12 year 1, \n",
    "## month 1 year 2, month 2 year 2, ... etc)\n",
    "\n",
    "print('Updating time indices')\n",
    "\n",
    "Tanom_da_list = []\n",
    "\n",
    "for month in range(1, 13):\n",
    "\n",
    "    Tanoms_month = Tanoms_bymonth[month]\n",
    "    Tanoms_month['time'] = Tanoms_month.time * 12 + month - 1\n",
    "    Tanoms_month = Tanoms_month.to_dataset(name='T')\n",
    "    Tanom_da_list.append(Tanoms_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82361c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatenate over time dimension and then sort by time \n",
    "print('Merging and sorting by time')\n",
    "\n",
    "InVERT_stacked = xr.concat(Tanom_da_list, dim='time').sortby('time')\n",
    "InVERT_stacked['gmean'] = areaweighted_mean(InVERT_stacked.T)\n",
    "InVERT_stacked.to_netcdf(savepath + 'InVERT_' + str(nmodes) + 'modes_stacked.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9456373",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate into ensemble members\n",
    "\n",
    "InVERT_T = unstack_time(InVERT_stacked, esize=n_samples)\n",
    "\n",
    "print('Saving final InVERT dataset')\n",
    "\n",
    "InVERT_T.to_netcdf(savepath + 'InVERT_'+str(nmodes)+'modes.nc')\n",
    "\n",
    "print('saved')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
